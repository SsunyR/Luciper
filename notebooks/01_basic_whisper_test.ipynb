{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a2ecea6",
   "metadata": {},
   "source": [
    "# Task 1.3: Basic Whisper Model Test\n",
    "\n",
    "This notebook demonstrates the basic functionality of loading a pre-trained OpenAI Whisper model using the Hugging Face `transformers` library and performing inference on a sample audio file to generate subtitles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adb2203",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import necessary libraries. Ensure `transformers`, `torch`, and an audio processing library like `librosa` or `soundfile` are installed. For simplicity with Hugging Face, we can also use `datasets` to load audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d414c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import librosa # Or soundfile, or datasets.Audio\n",
    "import torchaudio\n",
    "import IPython.display as ipd # For playing audio in notebook (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f53e142",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Define the model to use and a placeholder for the audio file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37ff5f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Define the model ID from Hugging Face Model Hub\n",
    "MODEL_ID = \"openai/whisper-tiny\" # Using tiny for a quick test, can be base, small, etc.\n",
    "\n",
    "# Placeholder for the sample audio file path\n",
    "# TODO: Replace this with an actual path to a .wav, .flac, or .mp3 file\n",
    "SAMPLE_AUDIO_PATH = \"../samples/sample_audio.mp3\" \n",
    "\n",
    "# Device configuration (use GPU if available)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35f113",
   "metadata": {},
   "source": [
    "## 3. Load Model and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99909bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea34c47d2a4465588a5bb71c4512413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85555be96084acab948c851b9a5f8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05771157f0346608510545606445df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66afbb005d454a14a6fea21b373c6217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf18c55b8aea4ac5ad10342fec33a228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d79e6b4f2a4854a7ba84701140fc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c82132caef2434c8e3a06e1b699538f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a968ab6bc147a483f2b690c086e3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66fb7d897a49413496715cb2257eab80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37c338989924d238de93a397543dd59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38c287bae2242ba864c0af0505e6ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model and processor for openai/whisper-tiny\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    processor = WhisperProcessor.from_pretrained(MODEL_ID)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "    print(f\"Successfully loaded model and processor for {MODEL_ID}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or processor: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f885507",
   "metadata": {},
   "source": [
    "## 4. Load and Preprocess Audio\n",
    "\n",
    "Load the audio file and preprocess it using the WhisperProcessor. Whisper expects audio to be single-channel (mono) and sampled at 16kHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b97d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_audio(audio_path: str, target_sr: int = 16000):\n",
    "    \"\"\"Loads an audio file, converts to mono, and resamples to target_sr.\"\"\"\n",
    "    try:\n",
    "        # Load audio file; librosa loads as float32, converts to mono, and resamples\n",
    "        speech_array, sampling_rate = librosa.load(audio_path, sr=target_sr, mono=True) \n",
    "        print(f\"Audio loaded. Original SR: {sampling_rate} (resampled to {target_sr}), Duration: {len(speech_array)/target_sr:.2f}s\")\n",
    "        # Display audio player in notebook (optional)\n",
    "        ipd.display(ipd.Audio(data=speech_array, rate=target_sr))\n",
    "        return speech_array, target_sr\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Audio file not found at {audio_path}. Please update SAMPLE_AUDIO_PATH.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing audio: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load and preprocess the sample audio\n",
    "# This will likely show a FileNotFoundError until SAMPLE_AUDIO_PATH is updated.\n",
    "speech_array, sampling_rate = load_and_preprocess_audio(SAMPLE_AUDIO_PATH)\n",
    "\n",
    "# Process the audio array to get input features\n",
    "input_features = None\n",
    "if speech_array is not None and processor is not None:\n",
    "    try:\n",
    "        input_features = processor(speech_array, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features.to(DEVICE)\n",
    "        print(\"Audio preprocessed into input features.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during feature extraction: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279dd20",
   "metadata": {},
   "source": [
    "## 5. Perform Inference (Generate Subtitles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeec9bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = \"\"\n",
    "if input_features is not None and model is not None and processor is not None:\n",
    "    try:\n",
    "        print(\"Generating transcription...\")\n",
    "        # Generate token IDs\n",
    "        predicted_ids = model.generate(input_features)\n",
    "        \n",
    "        # Decode token IDs to text\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "        \n",
    "        print(\"\\nTranscription:\")\n",
    "        if isinstance(transcription, list):\n",
    "            for i, t in enumerate(transcription):\n",
    "                print(f\"Segment {i+1}: {t.strip()}\")\n",
    "        else:\n",
    "            print(transcription.strip())\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription generation: {e}\")\n",
    "else:\n",
    "    print(\"Skipping transcription generation due to previous errors or missing components.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baff4b2",
   "metadata": {},
   "source": [
    "## 6. Document Results and Observations\n",
    "\n",
    "*(Manually fill this section after running the notebook)*\n",
    "\n",
    "*   **Model Used:** (e.g., `openai/whisper-tiny`)\n",
    "*   **Sample Audio File:** (Brief description or name if you replace the placeholder)\n",
    "*   **Generated Transcription:** (Paste the output here)\n",
    "*   **Observations:** (Any issues encountered, quality of transcription, time taken, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
